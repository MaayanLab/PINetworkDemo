{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess as sb\n",
    "from Bio import Entrez\n",
    "import pickle\n",
    "import os\n",
    "from Bio import Entrez\n",
    "import pickle\n",
    "import traceback\n",
    "import unittest\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create directories if they don't exist\n",
    "if not os.path.exists(\"./Data\"):\n",
    "    os.makedirs(\"./Data\")\n",
    "if not os.path.exists(\"./Network\"):\n",
    "    os.makedirs(\"./Network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining and formatting Award information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us declare a dataframe that will store all of the information from BRIMR 2024.\n",
    "whole_df = pd.read_excel(\"./data/Brimr_2024.xlsx\", header = 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that there are numerous columns, the columns that we care about are Organization, Project Number, PI Name, and Project Title. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the dataframe by institution of your choice: (We utilize Icahn School of Medicine by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you would like the <b>default</b> institution, MOUNT SINAI ICAHN SCHOOL OF MEDICINE run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution = \"MOUNT SINAI ICAHN SCHOOL OF MEDICINE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you would like a <b> random </b> institution instead run the cell below. \n",
    "<b>WARNING:</b> If you select this option there is the possibility of obtaining an empty network. \n",
    "Uncomment the cell below if you would like a random institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#institution = random.choice(whole_df['Organization'].unique())\n",
    "#print(institution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform filtering of the dataframe by selected institiution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = whole_df[whole_df['Organization'] == (institution)]\n",
    "filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered the dataframe by the given institution, let us filter out R&D contract awards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[filtered_df['FUNDING MECHANISM'] != 'R&D Contracts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a filtered the dataframe by institution and funding mechanism, let us remove unnecessary columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep Organization, Project Number, PI Name, and Project Title in the filtered dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[['Organization', 'PROJECT NUMBER', 'PI NAME', 'PROJECT TITLE']]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that we have 727 awards over the past year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Project Number and querying pubmed through biopython's entrez submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, by default the whole project number is not usable to query pubmed. In order to perform a pubmed search we need to use everything after the 4th character and before the dash. We can write a function to do this for us and ensure that we are correctly modifying the project number. More information on grant project numbers can be found here at [Understanding Grant Numbers](https://www.era.nih.gov/eraHelp/commons/Commons/understandGrantNums.htm?TocPath=Commons%20Basics%7C_____2). More information on how to search Pubmed utilizing grant numbers can be found here at [Pubmed User Guide](https://pubmed.ncbi.nlm.nih.gov/help/#gr).\n",
    "\n",
    "For example:\n",
    "- 5R01DK046865-31 -> DK046865\n",
    "- 1R21DK139543-01A1 -> DK139543\n",
    "- 5R01EY029736-05 -> EY029736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_pm(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Modifies a string from the BRIMR dataset to a format suitable for PubMed queries.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string, typically a grant number.\n",
    "\n",
    "    Returns:\n",
    "        str: A modified string for PubMed query, or None if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = s.split(\"-\")[0]\n",
    "        s = s[4:]\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "## This is for unit tests of the modify_pm function to ensure that it is doing what it should be.\n",
    "class TestModifyPM(unittest.TestCase):\n",
    "\n",
    "    def test_valid_inputs(self):\n",
    "        self.assertEqual(modify_pm(\"5R01DK046865-31\"), \"DK046865\")\n",
    "        self.assertEqual(modify_pm(\"1R21DK139543-01A1\"), \"DK139543\")\n",
    "        self.assertEqual(modify_pm(\"5R01EY029736-05\"), \"EY029736\")\n",
    "\n",
    "\n",
    "# Run the tests in Jupyter\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestModifyPM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the project number using the modify_pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the whole df so that PubMed is Queryable.\n",
    "filtered_df['PROJECT NUMBER BAK'] = filtered_df['PROJECT NUMBER']\n",
    "filtered_df['PROJECT NUMBER'] = filtered_df['PROJECT NUMBER'].apply(modify_pm)\n",
    "filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a last sanity check that all project numbers are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop_duplicates(subset=\"PROJECT NUMBER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the filtered_df into a file so that we can maintain the list of project numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"grants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pubmed Search\n",
    "Here we utilize the pubmed api to perform a search of all the pmid's associated with each grant number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first establish a few helper functions to help us out. We would like to establish the following helper functions:\n",
    "1. get_pmids : A function that will allow us to retrieve a list of pmids for each grant. \n",
    "2. get_authors : A function that will allow us to retrieve a list of authors for each grant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmids(grant_number: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Gets and returns all PMIDs for a given grant number.\n",
    "\n",
    "    Args:\n",
    "        grant_number (str): Grant number to search in PubMed.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of PMIDs (max 10,000) as strings.\n",
    "    \"\"\"\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=grant_number, retmax=10000)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    pmids = record.get(\"IdList\", [])\n",
    "    return pmids\n",
    "\n",
    "\n",
    "def get_records(pmids: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Gets and returns a list of PubMed records for the given PMIDs.\n",
    "\n",
    "    Args:\n",
    "        pmids (List[str]): A list of PMIDs from the PubMed database.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of records returned by Entrez.efetch.\n",
    "    \"\"\"\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), rettype=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    return records\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use Pubmed's entrez tool through the biopython wrapper. Please enter an email in the cell below to comply with the NCBI usage policy for this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"youremail@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set your email to comply with NCBI's usage policy\n",
    "Entrez.email = email  # Replace with your email address\n",
    "\n",
    "# Let us create a dictionary to store the associated PMID's with Awards.\n",
    "grant_pmids = {}\n",
    "\n",
    "# Iterate through the project numbers for the organization.\n",
    "for i, PM in enumerate(filtered_df.itertuples()):\n",
    "\n",
    "    # Print out grant processed and the project number.\n",
    "    print(f\"Processing grant {i + 1}: {PM[2]}\")\n",
    "    grant_number = PM[2]\n",
    "    year = PM[3]\n",
    "    \n",
    "    # Create the query for pubmed.\n",
    "    search_query = f'\"{grant_number}\"[gr]'\n",
    "    \n",
    "    try:\n",
    "        # Search for PMIDs associated with the grant\n",
    "        pmids = get_pmids(search_query)\n",
    "        \n",
    "        # Print out the associated grant number and pmids.\n",
    "        print(grant_number, pmids)\n",
    "\n",
    "        grant_pmids[grant_number] = pmids  # Store PMIDs for the grant.\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing grant {grant_number} for {org}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save results to a CSV file inside the organization's folder\n",
    "result_df = pd.DataFrame(list(grant_pmids.items()), columns=['Grant Number', 'PMIDs'])\n",
    "result_file = os.path.join(\"./\", 'grant_pmids.csv')\n",
    "result_df.to_csv(result_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained all of the relevant PMIDs, let us obtain metadata from each publication recorded, this includes the article's title, Journal, Author name and affiliation, and MeSH terms for each publication. We do this by using the following helper function. This helper function will retrieve 1000 publication's metadata at a time and associate it with each pubmed id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Iterable, Dict, Any\n",
    "from Bio import Entrez\n",
    "\n",
    "\n",
    "def get_metadata(pmids: Iterable[str]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve metadata for a given set of PMIDs using Entrez.\n",
    "\n",
    "    Parameters:\n",
    "        pmids (Iterable[str]): A collection of PubMed IDs (PMIDs) to retrieve metadata for.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Any]]: A dictionary where each key is a PMID and the value\n",
    "        is another dictionary containing:\n",
    "            - Title (str)\n",
    "            - Authors (str): semicolon-separated\n",
    "            - Affiliations (str): semicolon-separated\n",
    "            - Journal (str)\n",
    "            - Volume (str)\n",
    "            - Issue (str)\n",
    "            - Pages (str)\n",
    "            - Year (str)\n",
    "            - MeSH Terms (str): semicolon-separated\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    batch_size = 1000\n",
    "    pmid_list = list(pmids)\n",
    "\n",
    "    for i in range(0, len(pmid_list), batch_size):\n",
    "        batch_pmids = \",\".join(map(str, pmid_list[i:i + batch_size]))\n",
    "        print(\n",
    "            f\"Fetching metadata for PMIDs {i + 1} to \"\n",
    "            f\"{min(i + batch_size, len(pmid_list))}...\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            handle = Entrez.efetch(\n",
    "                db=\"pubmed\",\n",
    "                id=batch_pmids,\n",
    "                rettype=\"medline\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            records = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            print(\"Retrieved\")\n",
    "\n",
    "            for article in records[\"PubmedArticle\"]:\n",
    "                pmid = article[\"MedlineCitation\"][\"PMID\"]\n",
    "                citation = article[\"MedlineCitation\"]\n",
    "                article_data = citation[\"Article\"]\n",
    "                journal_info = article_data[\"Journal\"]\n",
    "\n",
    "                title = article_data.get(\"ArticleTitle\", \"N/A\")\n",
    "                journal = journal_info.get(\"Title\", \"N/A\")\n",
    "                issue = journal_info.get(\"JournalIssue\", {}).get(\"Issue\", \"N/A\")\n",
    "                volume = journal_info.get(\"JournalIssue\", {}).get(\"Volume\", \"N/A\")\n",
    "                year = journal_info.get(\"JournalIssue\", {}).get(\"PubDate\", {}).get(\"Year\", \"N/A\")\n",
    "                pages = article_data.get(\"Pagination\", {}).get(\"MedlinePgn\", \"N/A\")\n",
    "\n",
    "                authors = []\n",
    "                affiliations = []\n",
    "                if \"AuthorList\" in article_data:\n",
    "                    for author in article_data[\"AuthorList\"]:\n",
    "                        if \"LastName\" in author and \"ForeName\" in author:\n",
    "                            authors.append(f\"{author['ForeName']} {author['LastName']}\")\n",
    "                        if \"AffiliationInfo\" in author:\n",
    "                            affiliations.extend([\n",
    "                                aff[\"Affiliation\"]\n",
    "                                for aff in author[\"AffiliationInfo\"]\n",
    "                                if \"Affiliation\" in aff\n",
    "                            ])\n",
    "\n",
    "                mesh_terms = []\n",
    "                if \"MeshHeadingList\" in citation:\n",
    "                    mesh_terms = [\n",
    "                        mesh[\"DescriptorName\"]\n",
    "                        for mesh in citation[\"MeshHeadingList\"]\n",
    "                        if \"DescriptorName\" in mesh\n",
    "                    ]\n",
    "\n",
    "                metadata[pmid] = {\n",
    "                    \"Title\": title,\n",
    "                    \"Authors\": \"; \".join(authors),\n",
    "                    \"Affiliations\": \"; \".join(affiliations),\n",
    "                    \"Journal\": journal,\n",
    "                    \"Volume\": volume,\n",
    "                    \"Issue\": issue,\n",
    "                    \"Pages\": pages,\n",
    "                    \"Year\": year,\n",
    "                    \"MeSH Terms\": \"; \".join(mesh_terms)\n",
    "                }\n",
    "\n",
    "            print(\n",
    "                f\"Successfully retrieved metadata for batch {i + 1} to \"\n",
    "                f\"{min(i + batch_size, len(pmid_list))}.\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving metadata for PMIDs: {e}\")\n",
    "\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us actually retrieve the information using the helper function that we declared above. We save this information into a file that is delimited by |, as this is a unique character that is not frequently found in things like titles or names. (Titles sometimes have commas in their names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"grant_pmids.csv\")\n",
    "all_pmids = set()\n",
    "for pmid_list in df['PMIDs']:\n",
    "    pmid_list = eval(pmid_list) if isinstance(pmid_list, str) else pmid_list\n",
    "    all_pmids.update(pmid_list)\n",
    "print(f\"Collected {len(all_pmids)} unique PMIDs.\")\n",
    "\n",
    "# Retrieve metadata for all unique PMIDs\n",
    "metadata = get_metadata(all_pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out pmids that have a year < 2023 as we have awards from 2024.\n",
    "\n",
    "# Save metadata to a CSV file\n",
    "metadata_df = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "metadata_df['Year'] = pd.to_numeric(metadata_df['Year'].replace(\"N/A\", None), errors='coerce').astype(\"Int64\")\n",
    "metadata_df = metadata_df[metadata_df['Year'] >= 2024]\n",
    "metadata_file = os.path.join(\"./\", 'pmid_metadata.csv')\n",
    "metadata_df.to_csv(metadata_file, index_label='PMID', sep = \"|\")\n",
    "\n",
    "print(f\"Metadata retrieval complete. Results saved to {metadata_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of grants and authors let us process it into a network for Neo4J. We need to create a csv file for nodes and for edges. We will have four node types:\n",
    "- Authors <id, label>\n",
    "- Publications <id, label, Title, Journal, Authors>\n",
    "- Awards <id, label, Title, Contact>\n",
    "- MeSH <id, label> nodes. \n",
    "\n",
    "We will also write out the following edges:\n",
    "- Coauthors : Authors - Authors\n",
    "- Publications : Authors - Publications\n",
    "- MeSH : MeSH - Publications\n",
    "- Awards : Awards - Authors\n",
    "- Awards : Awards - Publications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Network Files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to curate a list of authors. One of the primary challenges of working with publication and authorship networks, is the problem of Author Name Disambiguation (AND), which is the task of identifying who is who. For example, someone may have entered their name on one publication as First Name, Middle Initial, Last Name, but on another publication wrote it as First Name, Middle Name, Last Name. This obviously is a complicated problem throughout bibliometrics, so for the sake of simplicity, we assume that all author's with the exact same names are the same, and authors that have the same middle initial and matching first and last names are also the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create our list of unique nodes of Authors. We can get this information by retrieving all of the contact pi names found in [grants.csv](./grants.csv) as well as the publication names stored in [pmid_metadata.csv](./pmid_metadata.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explode authors and affiliations into rows\n",
    "def explode_authors_affiliations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pmid = row.name  # assuming PMID is the index\n",
    "        authors = [a.strip() for a in str(row['Authors']).split(';') if a.strip()]\n",
    "        affiliations = [a.strip() for a in str(row['Affiliations']).split(';') if a.strip()]\n",
    "        \n",
    "        # Handle mismatch: align only up to the shortest length\n",
    "        for author, affiliation in zip(authors, affiliations):\n",
    "            records.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"Author\": author,\n",
    "                \"Affiliation\": affiliation\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "author_df = explode_authors_affiliations(metadata_df[['Authors', 'Affiliations']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to ensure that each author is within a given institution, so we need to ensure that affiliation contains a keyword. Please enter a keyword from the given institution {{institution}}. For the Icahn School of Medicine at Mount Sinai, the keyword would be ICAHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"ICAHN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df[author_df['Affiliation'].str.contains(keyword, case=False, na=False)]\n",
    "author_df.drop_duplicates(subset=\"Author\")\n",
    "del author_df['PMID']\n",
    "del author_df['Affiliation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add in contact pi's to the list if they are already non-existent. Let us extract out names from the Awards dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.loc[:, 'Cleaned PI Name'] = (\n",
    "    filtered_df['PI NAME']\n",
    "    .str.replace(\".\", \"\", regex=False)\n",
    "    .str.split(\",\")\n",
    "    .apply(lambda x: \" \".join(x[::-1]))\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "awards_authors = pd.DataFrame(filtered_df['Cleaned PI Name'])\n",
    "awards_authors.rename(columns={\n",
    "    'Cleaned PI Name': 'Author',\n",
    "}, inplace=True)\n",
    "authors_df = pd.concat([author_df, awards_authors])\n",
    "\n",
    "authors_df['label'] = authors_df['Author']\n",
    "authors_df = authors_df.drop(columns = [\"Author\"])\n",
    "\n",
    "authors_df['label'] = authors_df['label'].str.strip()\n",
    "authors_df = authors_df.drop_duplicates()\n",
    "authors_df.to_csv(os.path.join(\"./Network\", \"Authors.nodes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Publications Nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the publications data from the metadata dataframe that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df = metadata_df.copy()\n",
    "# Add label column from index\n",
    "publications_df['label'] = publications_df.index\n",
    "\n",
    "# Format Authors column: take first author and add \"et al.\"\n",
    "publications_df['Authors'] = (\n",
    "    publications_df['Authors']\n",
    "    .fillna('')\n",
    "    .apply(lambda x: (x.split(';')[0].strip() + ' et al.') if x else '')\n",
    ")\n",
    "\n",
    "# Safe column access and joining (with fallbacks for missing data)\n",
    "def format_journal_row(row):\n",
    "    parts = [\n",
    "        row.get('Journal', ''),\n",
    "        row.get('Volume', ''),\n",
    "        row.get('Issue', ''),\n",
    "        row.get('Pages', '')\n",
    "    ]\n",
    "    # Remove empty strings and join with commas\n",
    "    joined = \", \".join([part for part in parts if part])\n",
    "    year = row.get('Year', '')\n",
    "    return f\"{joined} ({year})\" if year else joined\n",
    "\n",
    "# Apply formatted journal info\n",
    "publications_df['Journal'] = publications_df.apply(format_journal_row, axis=1)\n",
    "publications_df = publications_df.drop(\n",
    "    columns=['Affiliations', 'Volume', 'Issue', 'Pages', 'Year', 'MeSH Terms'],\n",
    "    errors='ignore'  # in case any of them are already missing\n",
    ")\n",
    "publications_df = publications_df[['label', 'Title','Authors', 'Journal']]\n",
    "publications_df.to_csv(os.path.join(\"./Network\", \"Publications.nodes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pub = dict(zip(publications_df['label'], [True] * len(publications_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Awards nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_df = filtered_df\n",
    "awards_df = awards_df[['PROJECT NUMBER BAK', 'PROJECT TITLE', 'Cleaned PI Name']]\n",
    "awards_df = awards_df.rename(columns={\n",
    "    'PROJECT NUMBER BAK': 'label',\n",
    "    'PROJECT TITLE': 'Title',\n",
    "    'Cleaned PI Name': 'Contact'\n",
    "})\n",
    "awards_df.drop(\n",
    "    columns=['Organization', 'PROJECT NUMBER', 'PI NAME'],\n",
    "    errors='ignore'  # in case any of them are already missing\n",
    ")\n",
    "awards_df = awards_df[['label', 'Title', 'Contact']]\n",
    "awards_df.to_csv(os.path.join(\"./Network\", \"Awards.nodes.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MeSH Terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_df = metadata_df\n",
    "all_terms = (\n",
    "    metadata_df['MeSH Terms']\n",
    "    .dropna()\n",
    "    .str.split(';')                  # split each row on semicolons\n",
    "    .explode()                       # flatten into a single column\n",
    "    .str.strip()                     # remove extra spaces\n",
    "    .dropna()\n",
    "    .unique()                        # get unique values\n",
    ")\n",
    "\n",
    "# Convert to list if needed\n",
    "mesh_df = pd.DataFrame()\n",
    "mesh_df['label'] = sorted(all_terms.tolist())\n",
    "mesh_df = mesh_df[mesh_df['label'] != \"\"]\n",
    "mesh_df.to_csv(os.path.join(\"./Network\", \"MeSH.nodes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Coauthor: Author - Author edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "coauthors_edges = metadata_df.copy()\n",
    "coauthors_edges['pmid'] = coauthors_edges.index\n",
    "coauthors_edges = coauthors_edges[['Authors', 'pmid']]\n",
    "coauthors_edges\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is called `df` and the column is 'Authors'\n",
    "\n",
    "edges = []\n",
    "\n",
    "for pmid, row in coauthors_edges.iterrows():\n",
    "    if valid_pub.get(pmid, False):\n",
    "        authors = [a.strip() for a in str(row['Authors']).split(';') if a.strip()]\n",
    "        \n",
    "        # Create all pairwise combinations\n",
    "        for a1, a2 in itertools.combinations(authors, 2):\n",
    "            edges.append({'source label': a1, 'target label': a2})\n",
    "            edges.append({'source label': a2, 'target label': a1})  # Add reverse direction\n",
    "\n",
    "# Create the edge list DataFrame\n",
    "coauthors_edges_df = pd.DataFrame(edges)\n",
    "\n",
    "del coauthors_edges\n",
    "coauthors_edges_df.to_csv(os.path.join(\"./Network/\", \"Authors.Coauthors.Authors.edges.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Author - Publications edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for pmid, row in metadata_df.iterrows():\n",
    "    if not valid_pub.get(pmid, False):\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "    authors = [a.strip() for a in str(row['Authors']).split(';') if a.strip()]\n",
    "    \n",
    "    for author in authors:\n",
    "        edges.append({'source label': pmid, 'target label': author})\n",
    "\n",
    "# Create the edge list DataFrame\n",
    "edge_df = pd.DataFrame(edges)\n",
    "\n",
    "# Save to CSV with the required filename\n",
    "edge_df.to_csv(os.path.join('./Network', 'Publications.Publications.Authors.edges.csv'), index=False)\n",
    "\n",
    "\n",
    "cols = list(edge_df.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "\n",
    "edge_df = edge_df[cols]\n",
    "edge_df.to_csv(os.path.join('./Network/', \"Authors.Publications.Publications.edges.csv\"), index=False)\n",
    "\n",
    "authors_publications_edge_df = edge_df.copy()\n",
    "del edge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Awards edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Awards: Awards-Authors edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_edges = filtered_df[['PROJECT NUMBER BAK', 'Cleaned PI Name']]\n",
    "awards_edges = awards_edges.rename(columns = {\n",
    "    \"PROJECT NUMBER BAK\": \"source label\",\n",
    "    \"Cleaned PI Name\" : \"target label\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_edges.to_csv(os.path.join(\"./Network/\", \"Awards.Awards.Authors.edges.csv\"), index=False)\n",
    "cols = list(awards_edges.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "awards_edges = awards_edges[cols]\n",
    "awards_edges.to_csv(os.path.join(\"./Network/\", \"Authors.Awards.Awards.edges.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Awards-Publications edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'PROJECT NUMBER' in filtered_df to match 'Grant Number'\n",
    "filtered_df_renamed = filtered_df.rename(columns={'PROJECT NUMBER': 'Grant Number'})\n",
    "\n",
    "# Merge on 'Grant Number' and bring in the 'label'\n",
    "merged_df = result_df.merge(\n",
    "    filtered_df_renamed[['Grant Number', 'PROJECT NUMBER BAK']],\n",
    "    on='Grant Number',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Build directed edges: from award label to each PMID\n",
    "edges = []\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    source_label = row['PROJECT NUMBER BAK']\n",
    "    pmids = row['PMIDs']\n",
    "\n",
    "    if isinstance(pmids, list) and pmids:\n",
    "        for pmid in pmids:\n",
    "            if valid_pub.get(pmid, False):\n",
    "                edges.append({\n",
    "                    'source label': source_label,\n",
    "                    'target label': str(pmid)\n",
    "                })\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df.to_csv(os.path.join(\"./Network\", \"Awards.Awards.Publications.edges.csv\"), index=False)\n",
    "awards_publications_edge_df = edges_df\n",
    "del edges_df\n",
    "\n",
    "cols = list(awards_publications_edge_df.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "awards_publications_edge_df = awards_publications_edge_df[cols]\n",
    "awards_publications_edge_df.to_csv(os.path.join(\"./Network\", \"Publications.Awards.Awards.edges.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MeSH edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MeSH: MeSH - Publications edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_edges = []\n",
    "\n",
    "for pmid, row in metadata_df.iterrows():\n",
    "    mesh_terms = [m.strip() for m in str(row['MeSH Terms']).split(';') if m.strip()]\n",
    "    for mesh in mesh_terms:\n",
    "        if valid_pub.get(pmid, False):\n",
    "            mesh_edges.append({\n",
    "                'source label': pmid,\n",
    "                'target label': mesh\n",
    "            })\n",
    "\n",
    "publications_mesh_edges_df = pd.DataFrame(mesh_edges)\n",
    "publications_mesh_edges_df.to_csv(os.path.join(\"./Network/\",'Publications.MeSH.MeSH.edges.csv'), index=False)\n",
    "cols = list(publications_mesh_edges_df.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "publications_mesh_edges_df = publications_mesh_edges_df[cols]\n",
    "publications_mesh_edges_df.to_csv(os.path.join(\"./Network/\",'MeSH.MeSH.Publications.edges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format nodes for neo4j ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_folder = './Network'\n",
    "files = [f for f in os.listdir(network_folder) if f.endswith('.nodes.csv')]\n",
    "\n",
    "node_type_to_id_dict = {}\n",
    "label_to_id_dict = {}\n",
    "\n",
    "current_id = 1\n",
    "\n",
    "for file in files:\n",
    "    node_type = file.replace('.nodes.csv', '')\n",
    "    path = os.path.join(network_folder, file)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    label_to_id = {}\n",
    "    ids = []\n",
    "\n",
    "    for label in df['label']:\n",
    "        label = str(label).strip()\n",
    "\n",
    "        if 'Publications' in node_type:\n",
    "            # Publications use label directly as ID\n",
    "            ids.append(label)\n",
    "            if label not in label_to_id_dict:\n",
    "                label_to_id_dict[label] = label  # use label as ID\n",
    "        else:\n",
    "            if label not in label_to_id_dict:\n",
    "                label_to_id_dict[label] = current_id\n",
    "                label_to_id[label] = current_id\n",
    "                ids.append(current_id)\n",
    "                current_id += 1\n",
    "            else:\n",
    "                ids.append(label_to_id_dict[label])\n",
    "    \n",
    "    df['id'] = ids\n",
    "\n",
    "    # Save type-specific mapping only for non-Publication types\n",
    "    if 'Publications' not in node_type:\n",
    "        node_type_to_id_dict[node_type] = label_to_id\n",
    "\n",
    "    # Save updated node file\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_id_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to edge files\n",
    "network_folder = './Network'\n",
    "files = [f for f in os.listdir(network_folder) if f.endswith('.edges.csv')]\n",
    "\n",
    "# Assumes label_to_id_dict already exists\n",
    "# If not, you should load or build it before this step\n",
    "\n",
    "for file in files:\n",
    "    path = os.path.join(network_folder, file)\n",
    "    df = pd.read_csv(path)\n",
    "    # Rename label columns if they exist\n",
    "    if 'source label' in df.columns and 'target label' in df.columns:\n",
    "        df.rename(columns={\n",
    "            'source label': 'source_label',\n",
    "            'target label': 'target_label'\n",
    "        }, inplace=True)\n",
    "    df['source_label'] = df['source_label'].astype(str)\n",
    "    df['target_label'] = df['target_label'].astype(str)\n",
    "    # Add relation based on filename like A.B.C.edges.csv â†’ B\n",
    "    parts = file.split('.')\n",
    "    relation = parts[1] if len(parts) == 5 else 'Unknown'\n",
    "    df['relation'] = relation\n",
    "    # Map source and target IDs using label_to_id_dict\n",
    "    df['source'] = df['source_label'].map(label_to_id_dict)\n",
    "    df['target'] = df['target_label'].map(label_to_id_dict)\n",
    "\n",
    "\n",
    "    df = df[df['source'].notnull() & df['target'].notnull()]\n",
    "\n",
    "    # Save updated file\n",
    "    df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
