{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Network Construction Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to construct a PI network consisting of Authors, Publications, Awards, and MeSH Terms utilizing award information collected by BRIMR, which is then used to query pubmed using biopython's Entrez wrapper. This notebook is broken down into detailed steps in the protocol paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "import os\n",
    "from Bio import Entrez\n",
    "import itertools\n",
    "import unittest\n",
    "import random\n",
    "from typing import Iterable, Dict, Any\n",
    "from ast import literal_eval\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the required directories and folders in the cell below. This mainly consists of creating the construction/Network folder and the construction/Data folder. The construction/Network folder will eventually contain the required triples for ingestion into Neo4J and the construction/Data folder will contain the required BRIMR excel file which contains relevant award information and the required desc2025.xml file which is used to filter out the MeSH Headings that are too general. This corresponds to Protocol 1 - 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create directories if they don't exist\n",
    "\n",
    "# Create the construction/Data folder.\n",
    "if not os.path.exists(\"./Data\"):\n",
    "    os.makedirs(\"./Data\")\n",
    "\n",
    "# Create the construction/Network folder.\n",
    "if not os.path.exists(\"./Network\"):\n",
    "    os.makedirs(\"./Network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created two folders within our construction folder, let us now build out the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically a network data structure consists of nodes and edges. Nodes will be objects in the network, whereas edges are the relationships between those objects. In this demo network we choose to create a PI network, or a network that focuses on the relationships between principal investigators/contact authors that share publications. Our network will contain information on Authors, Publications, Awards, and MeSH terms. We obtain this data primarily from three different sources - BRIMR, RePORTER, and Pubmed, which cumulatively contain the information necessary to create the demo network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and format awards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now filter and format the awards dataframe. First let us read in our information from the past year from BRIMR. More information on how to obtain the information is described in Protocol 1 - 7 and 1-10. This section corresponds to Protocol 1-13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us declare a dataframe that will store all of the information from BRIMR 2024.\n",
    "# This may take a few seconds to load into memory.\n",
    "whole_df = pd.read_excel(\"./Data/Brimr_2024.xlsx\", header = 1)\n",
    "whole_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that there are numerous columns, although the columns that we care about are Organization, Project Number, PI Name, and Project Title. Now we would like to filter the awards by the institution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the dataframe by institution of your choice: (We utilize Icahn School of Medicine by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you would like the <b>default</b> institution, MOUNT SINAI ICAHN SCHOOL OF MEDICINE run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution = \"MOUNT SINAI ICAHN SCHOOL OF MEDICINE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you would like a <b> random </b> institution instead run the cell below.\n",
    "  \n",
    "<b>WARNING:</b> If you select this option there is the possibility of obtaining an empty network. \n",
    "Uncomment the cell below if you would like a random institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#institution = random.choice(whole_df['Organization'].unique())\n",
    "#print(institution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform filtering of the dataframe by selected institiution\n",
    "Here we filter the dataframe to ensure that only awards from a given instution are included in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe by the given institution.\n",
    "filtered_df = whole_df[whole_df['Organization'] == (institution)]\n",
    "filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filtered the dataframe by the given institution, let us filter out R&D contract awards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe by the given institution.\n",
    "filtered_df = filtered_df[filtered_df['FUNDING MECHANISM'] != 'R&D Contracts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a filtered the dataframe by institution and funding mechanism, let us remove unnecessary columns from the dataframe (i.e. Congressional District, City, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns.\n",
    "filtered_df = filtered_df[['Organization', 'PROJECT NUMBER', 'PI NAME', 'PROJECT TITLE']]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that we have ~727 awards over the past year from 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Formatting Project Number and querying pubmed through biopython's entrez submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, by default the whole project number is not usable to query pubmed. In order to perform a pubmed search we need to use everything after the 4th character and before the dash. We can write a function to do this for us and ensure that we are correctly modifying the project number. More information on grant project numbers can be found here at [Understanding Grant Numbers](https://www.era.nih.gov/eraHelp/commons/Commons/understandGrantNums.htm?TocPath=Commons%20Basics%7C_____2). More information on how to search Pubmed utilizing grant numbers can be found here at [Pubmed User Guide](https://pubmed.ncbi.nlm.nih.gov/help/#gr).\n",
    "\n",
    "For example:\n",
    "- 5R01DK046865-31 -> DK046865\n",
    "- 1R21DK139543-01A1 -> DK139543\n",
    "- 5R01EY029736-05 -> EY029736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to modify the project number to be queryable by pubmed.\n",
    "def modify_pm(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Modifies a string from the BRIMR dataset to a format suitable for PubMed queries.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string, typically a grant number.\n",
    "\n",
    "    Returns:\n",
    "        str: A modified string for PubMed query, or None if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = s.split(\"-\")[0]\n",
    "        s = s[4:]\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "## This is for unit tests of the modify_pm function to ensure that it is doing what it should be.\n",
    "class TestModifyPM(unittest.TestCase):\n",
    "\n",
    "    def test_valid_inputs(self):\n",
    "        self.assertEqual(modify_pm(\"5R01DK046865-31\"), \"DK046865\")\n",
    "        self.assertEqual(modify_pm(\"1R21DK139543-01A1\"), \"DK139543\")\n",
    "        self.assertEqual(modify_pm(\"5R01EY029736-05\"), \"EY029736\")\n",
    "\n",
    "\n",
    "# Run the tests in Jupyter\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestModifyPM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify the project number using the modify_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the whole df so that PubMed is queryable.\n",
    "filtered_df['PROJECT NUMBER BAK'] = filtered_df['PROJECT NUMBER']\n",
    "filtered_df['PROJECT NUMBER'] = filtered_df['PROJECT NUMBER'].apply(modify_pm)\n",
    "filtered_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a last sanity check that all project numbers are unique. That is we will remove all duplicate project number's so that we are not accidentally collecting the same project information twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop_duplicates(subset=\"PROJECT NUMBER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the filtered_df into a file so that we can maintain the list of project numbers. This should be ~684 awards that are contained in the dataframe. We write out the dataframe to be in \"./construction/grants.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"grants.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pubmed search\n",
    "Here we utilize the pubmed entrez tool to perform a search of all the pmid's associated with each grant number. We do this by utilizing biopython's entrez wrapper so that we can query PubMed programtically. This step corresponds to Protocol 1-13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first establish a few helper functions to help us out. We would like to establish the following helper functions:\n",
    "1. get_pmids : A function that will allow us to retrieve a list of pmids for each grant. \n",
    "2. get_authors : A function that will allow us to retrieve a list of authors for each grant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve a list of pmids for a given grant number.\n",
    "def get_pmids(grant_number: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Gets and returns all PMIDs for a given grant number.\n",
    "\n",
    "    Args:\n",
    "        grant_number (str): Grant number to search in PubMed.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of PMIDs (max 10,000) as strings.\n",
    "    \"\"\"\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=grant_number, retmax=10000)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    pmids = record.get(\"IdList\", [])\n",
    "    return pmids\n",
    "\n",
    "# Function to retrieve a list of PubMed records for the given PMIDs.\n",
    "def get_records(pmids: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Gets and returns a list of PubMed records for the given PMIDs.\n",
    "\n",
    "    Args:\n",
    "        pmids (List[str]): A list of PMIDs from the PubMed database.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of records returned by Entrez.efetch.\n",
    "    \"\"\"\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(pmids), rettype=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    return records\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use Pubmed's entrez tool through the biopython wrapper. Please enter an email in the cell below to comply with the NCBI usage policy for this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"youremail@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that an email has been entered, we need to search for all PubMed ID's or PMID's associated with each grant. We do this by constructing a search for each award and then add it to a dictionary storing this information. We then use this dictionary to create a Pandas DataFrame. We will then save the Pandas DataFrame to \"./construction/grant_pmids.csv\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set your email to comply with NCBI's usage policy\n",
    "Entrez.email = email  # Replace with your email address\n",
    "\n",
    "# Let us create a dictionary to store the associated PMID's with Awards.\n",
    "grant_pmids = {}\n",
    "\n",
    "# Iterate through the project numbers for the organization.\n",
    "for i, PM in enumerate(filtered_df.itertuples()):\n",
    "\n",
    "    # Print out grant processed and the project number.\n",
    "    print(f\"Processing grant {i + 1}: {PM[2]}\")\n",
    "    grant_number = PM[2]\n",
    "    year = PM[3]\n",
    "    \n",
    "    # Create the query for pubmed.\n",
    "    search_query = f'\"{grant_number}\"[gr]'\n",
    "    \n",
    "    try:\n",
    "        # Search for PMIDs associated with the grant\n",
    "        pmids = get_pmids(search_query)\n",
    "        \n",
    "        # Print out the associated grant number and pmids.\n",
    "        print(grant_number, pmids)\n",
    "\n",
    "        grant_pmids[grant_number] = pmids  # Store PMIDs for the grant.\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing grant {grant_number}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save results to a CSV file inside the organization's folder\n",
    "result_df = pd.DataFrame(list(grant_pmids.items()), columns=['Grant Number', 'PMIDs'])\n",
    "result_file = os.path.join(\"./\", 'grant_pmids.csv')\n",
    "result_df.to_csv(result_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve publication metadata from PubMed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained all of the relevant PMIDs, let us obtain metadata from each publication recorded, this includes the article's title, Journal, Author name and affiliation, and MeSH terms for each publication. We do this by using the following helper function. This helper function will retrieve 1000 publication's metadata at a time and associate it with each pubmed id. This section corresponds to Protocol 1-14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to retrieve metadata for a given set of PMIDs.\n",
    "def get_metadata(pmids: Iterable[str]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve metadata for a given set of PMIDs using Entrez.\n",
    "\n",
    "    Parameters:\n",
    "        pmids (Iterable[str]): A collection of PubMed IDs (PMIDs) to retrieve metadata for.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Any]]: A dictionary where each key is a PMID and the value\n",
    "        is another dictionary containing:\n",
    "            - Title (str)\n",
    "            - Authors (str): semicolon-separated\n",
    "            - Affiliations (str): semicolon-separated\n",
    "            - Journal (str)\n",
    "            - Volume (str)\n",
    "            - Issue (str)\n",
    "            - Pages (str)\n",
    "            - Year (str)\n",
    "            - MeSH Terms (str): semicolon-separated\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store metadata.\n",
    "    metadata = {}\n",
    "    # Set the batch size to 1000.\n",
    "    batch_size = 1000\n",
    "    # Convert the PMIDs to a list.\n",
    "    pmid_list = list(pmids)\n",
    "    # Iterate through the PMIDs in batches of 1000.\n",
    "    for i in range(0, len(pmid_list), batch_size):\n",
    "        batch_pmids = \",\".join(map(str, pmid_list[i:i + batch_size]))\n",
    "        print(\n",
    "            f\"Fetching metadata for PMIDs {i + 1} to \"\n",
    "            f\"{min(i + batch_size, len(pmid_list))}...\"\n",
    "        )\n",
    "        # Fetch the metadata for the given PMIDs.\n",
    "        try:\n",
    "            # Fetch the metadata for the given PMIDs.\n",
    "            handle = Entrez.efetch(\n",
    "                db=\"pubmed\",\n",
    "                id=batch_pmids,\n",
    "                rettype=\"medline\",\n",
    "                retmode=\"xml\"\n",
    "            )\n",
    "            # Read the metadata for the given PMIDs.\n",
    "            records = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            print(\"Retrieved\")\n",
    "            # Iterate through the records.\n",
    "            for article in records[\"PubmedArticle\"]:\n",
    "                pmid = article[\"MedlineCitation\"][\"PMID\"]\n",
    "                citation = article[\"MedlineCitation\"]\n",
    "                article_data = citation[\"Article\"]\n",
    "                journal_info = article_data[\"Journal\"]\n",
    "                # Get the title, journal, issue, volume, year, and pages of the article.\n",
    "                title = article_data.get(\"ArticleTitle\", \"N/A\")\n",
    "                journal = journal_info.get(\"Title\", \"N/A\")\n",
    "                issue = journal_info.get(\"JournalIssue\", {}).get(\"Issue\", \"N/A\")\n",
    "                volume = journal_info.get(\"JournalIssue\", {}).get(\"Volume\", \"N/A\")\n",
    "                year = journal_info.get(\"JournalIssue\", {}).get(\"PubDate\", {}).get(\"Year\", \"N/A\")\n",
    "                pages = article_data.get(\"Pagination\", {}).get(\"MedlinePgn\", \"N/A\")\n",
    "                # Get the authors and affiliations of the article.\n",
    "                authors = []\n",
    "                affiliations = []\n",
    "                if \"AuthorList\" in article_data:\n",
    "                    for author in article_data[\"AuthorList\"]:\n",
    "                        if \"LastName\" in author and \"ForeName\" in author:\n",
    "                            authors.append(f\"{author['ForeName']} {author['LastName']}\")\n",
    "                        if \"AffiliationInfo\" in author:\n",
    "                            affiliations.extend([\n",
    "                                aff[\"Affiliation\"]\n",
    "                                for aff in author[\"AffiliationInfo\"]\n",
    "                                if \"Affiliation\" in aff\n",
    "                            ])\n",
    "                # Get the MeSH terms of the article.\n",
    "                mesh_terms = []\n",
    "                if \"MeshHeadingList\" in citation:\n",
    "                    mesh_terms = [\n",
    "                        mesh[\"DescriptorName\"]\n",
    "                        for mesh in citation[\"MeshHeadingList\"]\n",
    "                        if \"DescriptorName\" in mesh\n",
    "                    ]\n",
    "                # Add the metadata to the dictionary.\n",
    "                metadata[pmid] = {\n",
    "                    \"Title\": title,\n",
    "                    \"Authors\": \"; \".join(authors),\n",
    "                    \"Affiliations\": \"; \".join(affiliations),\n",
    "                    \"Journal\": journal,\n",
    "                    \"Volume\": volume,\n",
    "                    \"Issue\": issue,\n",
    "                    \"Pages\": pages,\n",
    "                    \"Year\": year,\n",
    "                    \"MeSH Terms\": \"; \".join(mesh_terms)\n",
    "                }\n",
    "            # Print out the success message.\n",
    "            print(\n",
    "                f\"Successfully retrieved metadata for batch {i + 1} to \"\n",
    "                f\"{min(i + batch_size, len(pmid_list))}.\"\n",
    "            )\n",
    "        # Print out the error message.\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving metadata for PMIDs: {e}\")\n",
    "    # Return the metadata.\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us actually retrieve the information using the helper function that we declared above. We save this information into a file that is delimited by |, as this is a unique character that is not frequently found in things like titles or names. (Titles sometimes have commas in their names.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the grant_pmids.csv file.\n",
    "df = pd.read_csv(\"grant_pmids.csv\")\n",
    "# Initialize an empty set to store all unique PMIDs.\n",
    "all_pmids = set()\n",
    "# Iterate through the PMIDs in the dataframe.\n",
    "for pmid_list in df['PMIDs']:\n",
    "    pmid_list = eval(pmid_list) if isinstance(pmid_list, str) else pmid_list\n",
    "    all_pmids.update(pmid_list)\n",
    "print(f\"Collected {len(all_pmids)} unique PMIDs.\")\n",
    "\n",
    "# Retrieve metadata for all unique PMIDs\n",
    "metadata = get_metadata(all_pmids)\n",
    "\n",
    "\n",
    "# Save metadata to a CSV file\n",
    "\n",
    "metadata_df = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "\n",
    "# Filter out pmids that have a year < 2023 as we have awards from 2024.\n",
    "metadata_df['Year'] = pd.to_numeric(metadata_df['Year'].replace(\"N/A\", None), errors='coerce').astype(\"Int64\")\n",
    "metadata_df = metadata_df[metadata_df['Year'] >= 2024]\n",
    "\n",
    "\n",
    "metadata_file = os.path.join(\"./\", 'pmid_metadata.csv')\n",
    "metadata_df.to_csv(metadata_file, index_label='PMID', sep = \"|\")\n",
    "\n",
    "\n",
    "print(f\"Metadata retrieval complete. Results saved to {metadata_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of grants and authors let us process it into a network for Neo4J. We need to create a csv file for nodes and for edges. We will have four node types:\n",
    "- Authors <id, label>\n",
    "- Publications <id, label, Title, Journal, Authors>\n",
    "- Awards <id, label, Title, Contact>\n",
    "- MeSH <id, label> nodes. \n",
    "\n",
    "We will also write out the following edges:\n",
    "- Coauthors : Authors - Authors\n",
    "- Publications : Authors - Publications\n",
    "- MeSH : MeSH - Publications\n",
    "- Awards : Awards - Authors\n",
    "- Awards : Awards - Publications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"Authors\" node type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to curate a list of authors. One of the primary challenges of working with publication and authorship networks, is the problem of Author Name Disambiguation (AND), which is the task of identifying who is who. For example, someone may have entered their name on one publication as First Name, Middle Initial, Last Name, but on another publication wrote it as First Name, Middle Name, Last Name. This obviously is a complicated problem throughout bibliometrics, so for the sake of simplicity, we assume that all author's with the exact same names are the same, and authors that have the same middle initial and matching first and last names are also the same. Let us  create our list of unique nodes of Authors. We can get this information by retrieving all of the contact PI names found in [grants.csv](./grants.csv) as well as the publication names stored in [pmid_metadata.csv](./pmid_metadata.csv). This section creates the file \"./Network/Authors.nodes.csv\" and corresponds to Protocol 1 - 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explode authors and affiliations into rows\n",
    "def explode_authors_affiliations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pmid = row.name  # assuming PMID is the index\n",
    "        authors = [a.strip() for a in str(row['Authors']).split(';') if a.strip()]\n",
    "        affiliations = [a.strip() for a in str(row['Affiliations']).split(';') if a.strip()]\n",
    "        \n",
    "        # Handle mismatch: align only up to the shortest length\n",
    "        for author, affiliation in zip(authors, affiliations):\n",
    "            records.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"Author\": author,\n",
    "                \"Affiliation\": affiliation\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "author_df = explode_authors_affiliations(metadata_df[['Authors', 'Affiliations']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to ensure that each author is within a given institution, so we need to ensure that affiliation contains a keyword. Please enter a keyword from the given institution {{institution}}. For the Icahn School of Medicine at Mount Sinai, the keyword would be ICAHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"ICAHN\"\n",
    "author_df[author_df['Affiliation'].str.contains(keyword, case=False, na=False)]\n",
    "author_df.drop_duplicates(subset=\"Author\")\n",
    "del author_df['PMID']\n",
    "del author_df['Affiliation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to filter out all of the authors for only contact PIs so that we don't obtain unwanted authors in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean PI names from \"Last, First\" to \"First Last\" and format\n",
    "filtered_df = filtered_df.copy()\n",
    "filtered_df['Cleaned PI Name'] = (\n",
    "    filtered_df['PI NAME']\n",
    "    .fillna('')  # handle NaNs\n",
    "    .str.replace(\".\", \"\", regex=False)\n",
    "    .str.split(\",\")\n",
    "    .apply(lambda x: \" \".join(x[::-1]).strip() if isinstance(x, list) and len(x) > 1 else x[0] if x else \"\")\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "# Extract PI names into authors_df\n",
    "awards_authors = pd.DataFrame(filtered_df['Cleaned PI Name'].copy())\n",
    "awards_authors.rename(columns={'Cleaned PI Name': 'Author'}, inplace=True)\n",
    "\n",
    "# Build final authors node table (PIs only)\n",
    "authors_df = awards_authors.copy()\n",
    "authors_df['label'] = authors_df['Author'].str.strip()\n",
    "authors_df = authors_df.drop(columns=[\"Author\"])\n",
    "authors_df = authors_df.drop_duplicates()\n",
    "\n",
    "# Save to CSV\n",
    "authors_df.to_csv(os.path.join(\"./Network\", \"Authors.nodes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"Publications\" node type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the publications data from the metadata dataframe that we created earlier in the notebook. This section creates the file \"./Network/Publications.nodes.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "publications_df = metadata_df.copy()\n",
    "# Add label column from index\n",
    "publications_df['label'] = publications_df.index\n",
    "\n",
    "# Format Authors column: take first author and add \"et al.\"\n",
    "publications_df['Authors'] = (\n",
    "    publications_df['Authors']\n",
    "    .fillna('')\n",
    "    .apply(lambda x: (x.split(';')[0].strip() + ' et al.') if x else '')\n",
    ")\n",
    "\n",
    "# Safe column access and joining (with fallbacks for missing data)\n",
    "def format_journal_row(row):\n",
    "    parts = [\n",
    "        row.get('Journal', ''),\n",
    "        row.get('Volume', ''),\n",
    "        row.get('Issue', ''),\n",
    "        row.get('Pages', '')\n",
    "    ]\n",
    "    # Remove empty strings and join with commas\n",
    "    joined = \", \".join([part for part in parts if part])\n",
    "    year = row.get('Year', '')\n",
    "    return f\"{joined} ({year})\" if year else joined\n",
    "\n",
    "# Apply formatted journal info\n",
    "publications_df['Journal'] = publications_df.apply(format_journal_row, axis=1)\n",
    "publications_df = publications_df.drop(\n",
    "    columns=['Affiliations', 'Volume', 'Issue', 'Pages', 'Year', 'MeSH Terms'],\n",
    "    errors='ignore'  # in case any of them are already missing\n",
    ")\n",
    "publications_df = publications_df[['label', 'Title','Authors', 'Journal']]\n",
    "publications_df.to_csv(os.path.join(\"./Network\", \"Publications.nodes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to create a dictionary to ensure that the publications we are gathering are valid (i.e. a publication might be from the year 2023, which is after the award was granted/renewed). In the cases where a publication is not valid, we simply ignore it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pub = dict(zip(publications_df['label'], [True] * len(publications_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"Awards\" node type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the \"Awards\" node type. This simply involves collecting all of the awards that we used to collect publications and authors. This section creates the file \"./Network/Awards.nodes.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the filtered dataframe. \n",
    "awards_df = filtered_df.copy()\n",
    "\n",
    "# Only get relevant columns from the dataframe.\n",
    "awards_df = awards_df[['PROJECT NUMBER BAK', 'PROJECT TITLE', 'Cleaned PI Name']]\n",
    "\n",
    "# Rename the relevant columns.\n",
    "awards_df = awards_df.rename(columns={\n",
    "    'PROJECT NUMBER BAK': 'label',\n",
    "    'PROJECT TITLE': 'Title',\n",
    "    'Cleaned PI Name': 'Contact'\n",
    "})\n",
    "\n",
    "awards_df.drop(\n",
    "    columns=['Organization', 'PROJECT NUMBER', 'PI NAME'],\n",
    "    errors='ignore'  # in case any of them are already missing\n",
    ")\n",
    "\n",
    "# Create/format the node type for saving.\n",
    "awards_df = awards_df[['label', 'Title', 'Contact']]\n",
    "awards_df.to_csv(os.path.join(\"./Network\", \"Awards.nodes.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"MeSH\" node type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the \"MeSH\" node type. This is done by collecting information from the metadata DataFrame. We already have collected this information, we just need to assign an edge between each publication and its MeSH terms and filter out the high level MeSH terms by depth. We choose a depth level of 5 for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the metadata dataframe.\n",
    "mesh_df = metadata_df.copy()\n",
    "all_terms = (\n",
    "    metadata_df['MeSH Terms']\n",
    "    .dropna()\n",
    "    .str.split(';')                  # split each row on semicolons\n",
    "    .explode()                       # flatten into a single column\n",
    "    .str.strip()                     # remove extra spaces\n",
    "    .dropna()\n",
    "    .unique()                        # get unique values\n",
    ")\n",
    "\n",
    "# Convert to list if needed\n",
    "mesh_df = pd.DataFrame()\n",
    "mesh_df['label'] = sorted(all_terms.tolist())\n",
    "mesh_df = mesh_df[mesh_df['label'] != \"\"]\n",
    "\n",
    "with zipfile.ZipFile(\"./Data/desc2025.zip\", mode=\"r\") as z:\n",
    "    with z.open(\"desc2025.xml\") as xml_file:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "deep_mesh_terms = set()\n",
    "for descriptor in root.findall('.//DescriptorRecord'):\n",
    "    name_elem = descriptor.find('.//DescriptorName/String')\n",
    "    treenumbers = descriptor.findall('.//TreeNumberList/TreeNumber')\n",
    "    \n",
    "    # If any tree number has a dot, it's deeper than level 1\n",
    "    if any(tn.text.count(\".\") > 5 for tn in treenumbers):\n",
    "        if name_elem is not None:\n",
    "            deep_mesh_terms.add(name_elem.text.strip())\n",
    "\n",
    "# Step 2: Filter mesh_df\n",
    "mesh_df['label'] = mesh_df['label'].astype(str)\n",
    "mesh_df = mesh_df[mesh_df['label'].isin(deep_mesh_terms)]\n",
    "\n",
    "to_drop = ['Humans', 'Mice', 'Rats', \"New York City\",\n",
    "           'Prevalence', 'Medicare', 'Medicaid',\n",
    "           'Risk Factors', 'Mice, Inbred C57BL',\n",
    "           'Retrospective Studies', 'Longitudinal Studies',\n",
    "           'Prospective Studies', 'COVID-19', 'Mice, Knockout',\n",
    "           'Incidence', 'Risk Assessment', 'Vaccination',\n",
    "           'Antibodies, Viral', 'Mice, Transgenic',\n",
    "           'Antibodies, Neutralizing', 'Antibodies, Monoclonal']\n",
    "mesh_df = mesh_df[~mesh_df['label'].isin(to_drop)]\n",
    "# Create the node type for saving.\n",
    "mesh_df.to_csv(os.path.join(\"./Network\", \"MeSH.nodes.csv\"), index=False)\n",
    "\n",
    "# Create a list of valid mesh terms.\n",
    "valid_mesh = set(mesh_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Coauthor: Author - Author edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the edges for the network. The first type of edges we create are coauthors edges. We collect this information from the metadata DataFrame. We assign an edge between an author and another author if they share a valid publication. We then drop all duplicate edges. This is in accordance with Protocol 1 - 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_author_name(name: str) -> str:\n",
    "    # Normalize to \"Lastname Initial\" format\n",
    "    parts = re.split(r'\\s+', name.strip())\n",
    "    if len(parts) == 0:\n",
    "        return ''\n",
    "    lastname = parts[-1]\n",
    "    initials = ''.join([p[0] for p in parts[:-1] if p])  # e.g. \"John H.\" → \"JH\"\n",
    "    return f\"{lastname.lower()} {initials.lower()}\"\n",
    "\n",
    "def compare_author_names(author: str, list_authors: list[str]) -> bool:\n",
    "    target = normalize_author_name(author)\n",
    "    return target in list_authors\n",
    "\n",
    "def normalize_author_list(list_authors: list[str]) -> set[str]:\n",
    "    return set(normalize_author_name(a) for a in list_authors)\n",
    "\n",
    "\n",
    "coauthors_edges = metadata_df.copy()\n",
    "coauthors_edges['pmid'] = coauthors_edges.index\n",
    "coauthors_edges = coauthors_edges[['Authors', 'pmid']]\n",
    "coauthors_edges\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your DataFrame is called `df` and the column is 'Authors'\n",
    "\n",
    "edges = []\n",
    "\n",
    "valid_authors = normalize_author_list(authors_df['label'])\n",
    "for pmid, row in coauthors_edges.iterrows():\n",
    "    if valid_pub.get(pmid, False):\n",
    "        authors = [a.strip() for a in str(row['Authors']).split(';') if a.strip()]\n",
    "        \n",
    "        # Create all pairwise combinations\n",
    "        for a1, a2 in itertools.combinations(authors, 2):\n",
    "            if compare_author_names(a1, valid_authors) and compare_author_names(a2, valid_authors):\n",
    "                edges.append({'source label': a1, 'target label': a2})\n",
    "                edges.append({'source label': a2, 'target label': a1})  # Add reverse direction\n",
    "\n",
    "# Create the edge list DataFrame\n",
    "coauthors_edges_df = pd.DataFrame(edges)\n",
    "\n",
    "coauthors_edges_df.drop_duplicates(subset = [\"source label\", \"target label\"], inplace = True)\n",
    "\n",
    "del coauthors_edges\n",
    "coauthors_edges_df.to_csv(os.path.join(\"./Network/\", \"Authors.Coauthors.Authors.edges.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Author - Publications edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to create the edges between authors and their respective publications. We again utilize the metadata DataFrame we collected earlier to create edges between authors and their respective publications and drop duplicate edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for pmid, row in metadata_df.iterrows():\n",
    "    if not valid_pub.get(pmid, False):\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "    authors = [a.strip() for a in str(row['Authors']).split(';') if a.strip()]\n",
    "    \n",
    "    for author in authors:\n",
    "        if compare_author_names(author, valid_authors):\n",
    "            edges.append({'source label': pmid, 'target label': author})\n",
    "\n",
    "# Create the edge list DataFrame\n",
    "edge_df = pd.DataFrame(edges)\n",
    "\n",
    "# Drop duplicates.\n",
    "edge_df.drop_duplicates(subset = [\"source label\", \"target label\"], inplace = True)\n",
    "\n",
    "\n",
    "# Save to CSV with the required filename\n",
    "edge_df.to_csv(os.path.join('./Network', 'Publications.Publications.Authors.edges.csv'), index=False)\n",
    "\n",
    "\n",
    "cols = list(edge_df.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "\n",
    "edge_df = edge_df[cols]\n",
    "edge_df.to_csv(os.path.join('./Network/', \"Authors.Publications.Publications.edges.csv\"), index=False)\n",
    "\n",
    "authors_publications_edge_df = edge_df.copy()\n",
    "del edge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Awards edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to create edges between Awards and their contact principal investigators. Therefore we create edges between the \"Awards\" node type and the \"Authors\" node type. We also create edges between the \"Awards\" node type and the \"Publications\" node type. These edges represent the awards that are associated with particular publications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Awards: Awards-Authors edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the edges between the contact principal investigators and their awards. We do this by taking the original dataframe that we wrote out \"./grants.csv\" and utilizing the data their."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_edges = filtered_df[['PROJECT NUMBER BAK', 'Cleaned PI Name']].copy()\n",
    "awards_edges = awards_edges.rename(columns = {\n",
    "    \"PROJECT NUMBER BAK\": \"source label\",\n",
    "    \"Cleaned PI Name\" : \"target label\"\n",
    "})\n",
    "awards_edges.drop_duplicates(subset = [\"source label\", \"target label\"], inplace = True)\n",
    "awards_edges['target label'] = awards_edges['target label'].str.strip()\n",
    "awards_edges.to_csv(os.path.join(\"./Network/\", \"Awards.Awards.Authors.edges.csv\"), index=False)\n",
    "cols = list(awards_edges.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "awards_edges = awards_edges[cols]\n",
    "awards_edges.to_csv(os.path.join(\"./Network/\", \"Authors.Awards.Awards.edges.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Awards-Publications edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the edges between the \"Awards\" node type and the \"Publications\" node type. These edges represent the awards associated with each Publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'PROJECT NUMBER' in filtered_df to match 'Grant Number'\n",
    "result_df = pd.read_csv(\"grant_pmids.csv\")\n",
    "filtered_df_renamed = filtered_df.rename(columns={'PROJECT NUMBER': 'Grant Number'})\n",
    "\n",
    "# Merge on 'Grant Number' and bring in the 'label'\n",
    "merged_df = result_df.merge(\n",
    "    filtered_df_renamed[['Grant Number', 'PROJECT NUMBER BAK']],\n",
    "    on='Grant Number',\n",
    "    how='inner'\n",
    ")\n",
    "print(merged_df)\n",
    "# Build directed edges: from award label to each PMID\n",
    "edges = []\n",
    "\n",
    "merged_df['PMIDs'] = merged_df['PMIDs'].apply(literal_eval)\n",
    "for _, row in merged_df.iterrows():\n",
    "    source_label = row['PROJECT NUMBER BAK']\n",
    "    pmids = row['PMIDs']\n",
    "\n",
    "    if isinstance(pmids, list) and pmids:\n",
    "        for pmid in pmids:\n",
    "            print(pmid)\n",
    "            if valid_pub.get(pmid, False):\n",
    "                edges.append({\n",
    "                    'source label': source_label,\n",
    "                    'target label': str(pmid)\n",
    "                })\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(edges)\n",
    "# Convert to DataFrame and export\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df.drop_duplicates(subset = [\"source label\", \"target label\"], inplace = True)\n",
    "edges_df.to_csv(os.path.join(\"./Network\", \"Awards.Awards.Publications.edges.csv\"), index=False)\n",
    "awards_publications_edge_df = edges_df\n",
    "del edges_df\n",
    "print(awards_publications_edge_df)\n",
    "cols = list(awards_publications_edge_df.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "awards_publications_edge_df = awards_publications_edge_df[cols]\n",
    "awards_publications_edge_df.to_csv(os.path.join(\"./Network\", \"Publications.Awards.Awards.edges.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MeSH: MeSH - Publications edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create edges between the \"MeSH\" node type and the \"Publications\" node type. These edges represent how each publication has a set of \"MeSH\" terms that are associated with each Publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_edges = []\n",
    "\n",
    "for pmid, row in metadata_df.iterrows():\n",
    "    mesh_terms = [m.strip() for m in str(row['MeSH Terms']).split(';') if m.strip()]\n",
    "    for mesh in mesh_terms:\n",
    "        if valid_pub.get(pmid, False) and mesh in valid_mesh:\n",
    "            mesh_edges.append({\n",
    "                'source label': pmid,\n",
    "                'target label': mesh\n",
    "            })\n",
    "\n",
    "publications_mesh_edges_df = pd.DataFrame(mesh_edges)\n",
    "publications_mesh_edges_df.drop_duplicates(subset = [\"source label\", \"target label\"], inplace = True)\n",
    "publications_mesh_edges_df.to_csv(os.path.join(\"./Network/\",'Publications.MeSH.MeSH.edges.csv'), index=False)\n",
    "cols = list(publications_mesh_edges_df.columns)\n",
    "i, j = cols.index('source label'), cols.index('target label')\n",
    "cols[i], cols[j] = cols[j], cols[i]\n",
    "publications_mesh_edges_df = publications_mesh_edges_df[cols]\n",
    "publications_mesh_edges_df.to_csv(os.path.join(\"./Network/\",'MeSH.MeSH.Publications.edges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format files for neo4j ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to format files for neo4j ingestion. We do the actual ingestion with the script in \"./PINetwork/src/import_csv.py\" and \"./PINetwork/src/import_csv_with_merge.py\" in a later step, but that step requires that all *.nodes.csv files have the fields id and label which are unique and *.edges.csv which have the fields source, relation, and target which correspond to the id column for each node. We have previously in Protocol 1 - 15 and 1 - 16 wrote out nodes and edges with just label, source label, and target label. Now we need to create unique ids for each node type and use the proper fields for the edge files. This is in accordance with Protocol 1 - 17 and 1 - 18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format nodes for neo4j ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create/format the nodes files for ingestion. This involves creating a dictionary that maps each label to a unique id number and then writes it out the id field. We use this dictionary when we format the edges. This is in accordance with Protocol 1 - 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_folder = './Network'\n",
    "files = [f for f in os.listdir(network_folder) if f.endswith('.nodes.csv')]\n",
    "\n",
    "node_type_to_id_dict = {}\n",
    "label_to_id_dict = {}\n",
    "\n",
    "current_id = 1\n",
    "\n",
    "for file in files:\n",
    "    node_type = file.replace('.nodes.csv', '')\n",
    "    path = os.path.join(network_folder, file)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    label_to_id = {}\n",
    "    ids = []\n",
    "\n",
    "    for label in df['label']:\n",
    "        label = str(label).strip()\n",
    "\n",
    "        if 'Publications' in node_type:\n",
    "            # Publications use label directly as ID\n",
    "            ids.append(label)\n",
    "            if label not in label_to_id_dict:\n",
    "                label_to_id_dict[label] = label  # use label as ID\n",
    "        else:\n",
    "            if label not in label_to_id_dict:\n",
    "                label_to_id_dict[label] = current_id\n",
    "                label_to_id[label] = current_id\n",
    "                ids.append(current_id)\n",
    "                current_id += 1\n",
    "            else:\n",
    "                ids.append(label_to_id_dict[label])\n",
    "    \n",
    "    df['id'] = ids\n",
    "\n",
    "    # Save type-specific mapping only for non-Publication types\n",
    "    if 'Publications' not in node_type:\n",
    "        node_type_to_id_dict[node_type] = label_to_id\n",
    "\n",
    "    # Save updated node file\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we format the edge files. That is we rename the fields source label to be source_label and target label to be target_label. We also create the source, relation, and target fields in this cell. This is in accordance with Protocol 1 - 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to edge files\n",
    "network_folder = './Network'\n",
    "files = [f for f in os.listdir(network_folder) if f.endswith('.edges.csv')]\n",
    "\n",
    "# Assumes label_to_id_dict already exists\n",
    "# If not, you should load or build it before this step\n",
    "\n",
    "for file in files:\n",
    "    print(file)\n",
    "    path = os.path.join(network_folder, file)\n",
    "    df = pd.read_csv(path)\n",
    "    # Rename label columns if they exist\n",
    "    if 'source label' in df.columns and 'target label' in df.columns:\n",
    "        df.rename(columns={\n",
    "            'source label': 'source_label',\n",
    "            'target label': 'target_label'\n",
    "        }, inplace=True)\n",
    "    df['source_label'] = df['source_label'].astype(str)\n",
    "    df['target_label'] = df['target_label'].astype(str)\n",
    "    # Add relation based on filename like A.B.C.edges.csv → B\n",
    "    parts = file.split('.')\n",
    "    relation = parts[1] if len(parts) == 5 else 'Unknown'\n",
    "    df['relation'] = relation\n",
    "    # Map source and target IDs using label_to_id_dict\n",
    "    df['source'] = df['source_label'].map(label_to_id_dict)\n",
    "    df['target'] = df['target_label'].map(label_to_id_dict)\n",
    "\n",
    "\n",
    "    df = df[df['source'].notnull() & df['target'].notnull()]\n",
    "\n",
    "    # Save updated file\n",
    "    df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol 1 - 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a merged network file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a merged network of edges that we can optionally load into Cytoscape later to visually inspect the whole network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all edge files in construction.\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir(\"./Network/\"):\n",
    "    if file.endswith(\".edges.csv\"):\n",
    "        df = pd.concat([df, pd.read_csv(f\"./Network/{file}\")])\n",
    "df.to_csv(\"./merged_network.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
